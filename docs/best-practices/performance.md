# æ€§èƒ½ä¼˜åŒ–å»ºè®®

æœ¬æ–‡æ¡£æä¾›äº†Pythonåç«¯åº”ç”¨æ€§èƒ½ä¼˜åŒ–çš„æœ€ä½³å®è·µå’Œå…·ä½“å®æ–½æ–¹æ¡ˆï¼Œå¸®åŠ©å¼€å‘è€…æ„å»ºé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„åº”ç”¨ç¨‹åºã€‚

## ğŸ“‹ ç›®å½•

- [æ€§èƒ½ä¼˜åŒ–åŸåˆ™](#æ€§èƒ½ä¼˜åŒ–åŸåˆ™)
- [æ•°æ®åº“ä¼˜åŒ–](#æ•°æ®åº“ä¼˜åŒ–)
- [ç¼“å­˜ç­–ç•¥](#ç¼“å­˜ç­–ç•¥)
- [å¼‚æ­¥ç¼–ç¨‹](#å¼‚æ­¥ç¼–ç¨‹)
- [å†…å­˜ä¼˜åŒ–](#å†…å­˜ä¼˜åŒ–)
- [ç½‘ç»œä¼˜åŒ–](#ç½‘ç»œä¼˜åŒ–)
- [ä»£ç ä¼˜åŒ–](#ä»£ç ä¼˜åŒ–)
- [ç›‘æ§ä¸åˆ†æ](#ç›‘æ§ä¸åˆ†æ)
- [éƒ¨ç½²ä¼˜åŒ–](#éƒ¨ç½²ä¼˜åŒ–)
- [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•)

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–åŸåˆ™

### 1. æ€§èƒ½ä¼˜åŒ–çš„é»„é‡‘æ³•åˆ™

```python
# æ€§èƒ½ä¼˜åŒ–ä¼˜å…ˆçº§
class PerformanceOptimizationPrinciples:
    """
    æ€§èƒ½ä¼˜åŒ–åŸåˆ™
    1. æµ‹é‡ä¼˜å…ˆ - å…ˆæµ‹é‡ï¼Œå†ä¼˜åŒ–
    2. ç“¶é¢ˆè¯†åˆ« - æ‰¾åˆ°çœŸæ­£çš„æ€§èƒ½ç“¶é¢ˆ
    3. æ¸è¿›ä¼˜åŒ– - é€æ­¥ä¼˜åŒ–ï¼Œé¿å…è¿‡åº¦ä¼˜åŒ–
    4. æƒè¡¡å–èˆ - å¹³è¡¡æ€§èƒ½ã€å¯ç»´æŠ¤æ€§å’Œå¼€å‘æˆæœ¬
    """
    
    @staticmethod
    def measure_first():
        """æµ‹é‡ä¼˜å…ˆåŸåˆ™"""
        return {
            "profile_before_optimize": "ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·æµ‹é‡å½“å‰æ€§èƒ½",
            "identify_bottlenecks": "è¯†åˆ«çœŸæ­£çš„æ€§èƒ½ç“¶é¢ˆ",
            "set_performance_goals": "è®¾å®šæ˜ç¡®çš„æ€§èƒ½ç›®æ ‡",
            "measure_after_optimize": "ä¼˜åŒ–åé‡æ–°æµ‹é‡éªŒè¯æ•ˆæœ"
        }
    
    @staticmethod
    def optimization_priorities():
        """ä¼˜åŒ–ä¼˜å…ˆçº§"""
        return [
            "1. ç®—æ³•å’Œæ•°æ®ç»“æ„ä¼˜åŒ–",
            "2. æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–",
            "3. ç¼“å­˜ç­–ç•¥å®æ–½",
            "4. å¼‚æ­¥å¤„ç†ä¼˜åŒ–",
            "5. å†…å­˜ä½¿ç”¨ä¼˜åŒ–",
            "6. ç½‘ç»œä¼ è¾“ä¼˜åŒ–",
            "7. ä»£ç çº§åˆ«ä¼˜åŒ–"
        ]

# æ€§èƒ½ç›‘æ§è£…é¥°å™¨
import time
import functools
import logging
from typing import Callable, Any

def performance_monitor(func_name: str = None):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs) -> Any:
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                execution_time = time.time() - start_time
                logger = logging.getLogger('performance')
                logger.info(
                    f"Function {func_name or func.__name__} "
                    f"executed in {execution_time:.4f} seconds"
                )
        
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs) -> Any:
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                execution_time = time.time() - start_time
                logger = logging.getLogger('performance')
                logger.info(
                    f"Function {func_name or func.__name__} "
                    f"executed in {execution_time:.4f} seconds"
                )
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@performance_monitor("user_data_processing")
async def process_user_data(user_id: int):
    # å¤„ç†ç”¨æˆ·æ•°æ®çš„é€»è¾‘
    pass
```

### 2. æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
import asyncio
import time
from typing import List, Dict, Any, Callable
from dataclasses import dataclass
from statistics import mean, median, stdev

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    function_name: str
    total_runs: int
    total_time: float
    avg_time: float
    median_time: float
    min_time: float
    max_time: float
    std_dev: float
    requests_per_second: float

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·"""
    
    def __init__(self):
        self.results: List[BenchmarkResult] = []
    
    async def benchmark_async_function(
        self, 
        func: Callable, 
        args: tuple = (), 
        kwargs: dict = None, 
        runs: int = 100
    ) -> BenchmarkResult:
        """å¼‚æ­¥å‡½æ•°åŸºå‡†æµ‹è¯•"""
        kwargs = kwargs or {}
        execution_times = []
        
        # é¢„çƒ­
        for _ in range(min(10, runs // 10)):
            await func(*args, **kwargs)
        
        # æ­£å¼æµ‹è¯•
        start_total = time.time()
        for _ in range(runs):
            start_time = time.time()
            await func(*args, **kwargs)
            execution_times.append(time.time() - start_time)
        total_time = time.time() - start_total
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        result = BenchmarkResult(
            function_name=func.__name__,
            total_runs=runs,
            total_time=total_time,
            avg_time=mean(execution_times),
            median_time=median(execution_times),
            min_time=min(execution_times),
            max_time=max(execution_times),
            std_dev=stdev(execution_times) if len(execution_times) > 1 else 0,
            requests_per_second=runs / total_time
        )
        
        self.results.append(result)
        return result
    
    def benchmark_sync_function(
        self, 
        func: Callable, 
        args: tuple = (), 
        kwargs: dict = None, 
        runs: int = 100
    ) -> BenchmarkResult:
        """åŒæ­¥å‡½æ•°åŸºå‡†æµ‹è¯•"""
        kwargs = kwargs or {}
        execution_times = []
        
        # é¢„çƒ­
        for _ in range(min(10, runs // 10)):
            func(*args, **kwargs)
        
        # æ­£å¼æµ‹è¯•
        start_total = time.time()
        for _ in range(runs):
            start_time = time.time()
            func(*args, **kwargs)
            execution_times.append(time.time() - start_time)
        total_time = time.time() - start_total
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        result = BenchmarkResult(
            function_name=func.__name__,
            total_runs=runs,
            total_time=total_time,
            avg_time=mean(execution_times),
            median_time=median(execution_times),
            min_time=min(execution_times),
            max_time=max(execution_times),
            std_dev=stdev(execution_times) if len(execution_times) > 1 else 0,
            requests_per_second=runs / total_time
        )
        
        self.results.append(result)
        return result
    
    def print_results(self):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        for result in self.results:
            print(f"\n=== {result.function_name} ===")
            print(f"æ€»è¿è¡Œæ¬¡æ•°: {result.total_runs}")
            print(f"æ€»æ—¶é—´: {result.total_time:.4f}s")
            print(f"å¹³å‡æ—¶é—´: {result.avg_time:.4f}s")
            print(f"ä¸­ä½æ•°æ—¶é—´: {result.median_time:.4f}s")
            print(f"æœ€å°æ—¶é—´: {result.min_time:.4f}s")
            print(f"æœ€å¤§æ—¶é—´: {result.max_time:.4f}s")
            print(f"æ ‡å‡†å·®: {result.std_dev:.4f}s")
            print(f"æ¯ç§’è¯·æ±‚æ•°: {result.requests_per_second:.2f} RPS")

# ä½¿ç”¨ç¤ºä¾‹
async def example_usage():
    benchmark = PerformanceBenchmark()
    
    # æµ‹è¯•å¼‚æ­¥å‡½æ•°
    async def async_task():
        await asyncio.sleep(0.01)
        return "done"
    
    result = await benchmark.benchmark_async_function(async_task, runs=50)
    benchmark.print_results()
```

## ğŸ—„ï¸ æ•°æ®åº“ä¼˜åŒ–

### 1. æŸ¥è¯¢ä¼˜åŒ–

```python
from sqlalchemy import text, func, and_, or_
from sqlalchemy.orm import joinedload, selectinload, subqueryload
from typing import List, Optional, Dict, Any

class DatabaseOptimization:
    """æ•°æ®åº“ä¼˜åŒ–å·¥å…·ç±»"""
    
    def __init__(self, session):
        self.session = session
    
    def optimized_pagination(
        self, 
        model, 
        page: int = 1, 
        per_page: int = 20,
        filters: Dict[str, Any] = None
    ):
        """ä¼˜åŒ–çš„åˆ†é¡µæŸ¥è¯¢"""
        query = self.session.query(model)
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if filters:
            for key, value in filters.items():
                if hasattr(model, key):
                    query = query.filter(getattr(model, key) == value)
        
        # ä½¿ç”¨offsetå’Œlimitè¿›è¡Œåˆ†é¡µ
        offset = (page - 1) * per_page
        
        # è·å–æ€»æ•°ï¼ˆä¼˜åŒ–ï¼šåªåœ¨éœ€è¦æ—¶è®¡ç®—ï¼‰
        total = query.count()
        
        # è·å–æ•°æ®
        items = query.offset(offset).limit(per_page).all()
        
        return {
            'items': items,
            'total': total,
            'page': page,
            'per_page': per_page,
            'pages': (total + per_page - 1) // per_page
        }
    
    def cursor_based_pagination(
        self, 
        model, 
        cursor_field: str = 'id',
        cursor_value: Any = None,
        limit: int = 20,
        direction: str = 'next'
    ):
        """åŸºäºæ¸¸æ ‡çš„åˆ†é¡µï¼ˆé€‚åˆå¤§æ•°æ®é›†ï¼‰"""
        query = self.session.query(model)
        
        if cursor_value is not None:
            cursor_column = getattr(model, cursor_field)
            if direction == 'next':
                query = query.filter(cursor_column > cursor_value)
            else:
                query = query.filter(cursor_column < cursor_value)
        
        # æ’åº
        cursor_column = getattr(model, cursor_field)
        if direction == 'next':
            query = query.order_by(cursor_column.asc())
        else:
            query = query.order_by(cursor_column.desc())
        
        items = query.limit(limit + 1).all()
        
        has_more = len(items) > limit
        if has_more:
            items = items[:-1]
        
        next_cursor = None
        if items and has_more:
            next_cursor = getattr(items[-1], cursor_field)
        
        return {
            'items': items,
            'next_cursor': next_cursor,
            'has_more': has_more
        }
    
    def bulk_operations(self, model, data_list: List[Dict[str, Any]]):
        """æ‰¹é‡æ“ä½œä¼˜åŒ–"""
        # æ‰¹é‡æ’å…¥
        if data_list:
            self.session.bulk_insert_mappings(model, data_list)
            self.session.commit()
    
    def optimized_joins(self, model, relationships: List[str]):
        """ä¼˜åŒ–çš„å…³è”æŸ¥è¯¢"""
        query = self.session.query(model)
        
        for relationship in relationships:
            # ä½¿ç”¨joinedloadè¿›è¡Œé¢„åŠ è½½ï¼Œå‡å°‘N+1æŸ¥è¯¢é—®é¢˜
            query = query.options(joinedload(relationship))
        
        return query
    
    def raw_sql_optimization(self, sql: str, params: Dict[str, Any] = None):
        """åŸç”ŸSQLä¼˜åŒ–æŸ¥è¯¢"""
        # å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œä½¿ç”¨åŸç”ŸSQLå¯èƒ½æ›´é«˜æ•ˆ
        result = self.session.execute(text(sql), params or {})
        return result.fetchall()

# æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

class OptimizedDatabaseManager:
    """ä¼˜åŒ–çš„æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, database_url: str):
        self.engine = create_engine(
            database_url,
            # è¿æ¥æ± é…ç½®
            poolclass=QueuePool,
            pool_size=20,          # è¿æ¥æ± å¤§å°
            max_overflow=30,       # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
            pool_timeout=30,       # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
            pool_recycle=3600,     # è¿æ¥å›æ”¶æ—¶é—´
            pool_pre_ping=True,    # è¿æ¥å‰pingæ£€æŸ¥
            
            # æŸ¥è¯¢ä¼˜åŒ–
            echo=False,            # ç”Ÿäº§ç¯å¢ƒå…³é—­SQLæ—¥å¿—
            future=True,           # ä½¿ç”¨æ–°çš„API
            
            # è¿æ¥å‚æ•°
            connect_args={
                "connect_timeout": 10,
                "application_name": "MyApp",
            }
        )
    
    def get_session(self):
        """è·å–æ•°æ®åº“ä¼šè¯"""
        from sqlalchemy.orm import sessionmaker
        Session = sessionmaker(bind=self.engine)
        return Session()

# æŸ¥è¯¢ç¼“å­˜
from functools import lru_cache
import hashlib
import json

class QueryCache:
    """æŸ¥è¯¢ç»“æœç¼“å­˜"""
    
    def __init__(self, redis_client=None, default_ttl: int = 300):
        self.redis = redis_client
        self.default_ttl = default_ttl
        self.local_cache = {}
    
    def _generate_cache_key(self, query_str: str, params: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            'query': query_str,
            'params': params
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return f"query_cache:{hashlib.md5(cache_str.encode()).hexdigest()}"
    
    async def get_cached_result(self, query_str: str, params: Dict[str, Any]):
        """è·å–ç¼“å­˜ç»“æœ"""
        cache_key = self._generate_cache_key(query_str, params)
        
        if self.redis:
            cached_data = await self.redis.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
        else:
            return self.local_cache.get(cache_key)
        
        return None
    
    async def cache_result(
        self, 
        query_str: str, 
        params: Dict[str, Any], 
        result: Any, 
        ttl: int = None
    ):
        """ç¼“å­˜æŸ¥è¯¢ç»“æœ"""
        cache_key = self._generate_cache_key(query_str, params)
        ttl = ttl or self.default_ttl
        
        # åºåˆ—åŒ–ç»“æœ
        serialized_result = json.dumps(result, default=str)
        
        if self.redis:
            await self.redis.setex(cache_key, ttl, serialized_result)
        else:
            self.local_cache[cache_key] = result
    
    def cache_query(self, ttl: int = None):
        """æŸ¥è¯¢ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_cache_key(
                    func.__name__, 
                    {'args': args, 'kwargs': kwargs}
                )
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = await self.get_cached_result(
                    func.__name__, 
                    {'args': args, 'kwargs': kwargs}
                )
                
                if cached_result is not None:
                    return cached_result
                
                # æ‰§è¡ŒæŸ¥è¯¢
                result = await func(*args, **kwargs)
                
                # ç¼“å­˜ç»“æœ
                await self.cache_result(
                    func.__name__, 
                    {'args': args, 'kwargs': kwargs}, 
                    result, 
                    ttl
                )
                
                return result
            return wrapper
        return decorator
```

### 2. ç´¢å¼•ä¼˜åŒ–

```python
from sqlalchemy import Index, text
from typing import List, Dict, Any

class IndexOptimization:
    """ç´¢å¼•ä¼˜åŒ–å·¥å…·"""
    
    def __init__(self, session):
        self.session = session
    
    def analyze_query_performance(self, sql: str, params: Dict[str, Any] = None):
        """åˆ†ææŸ¥è¯¢æ€§èƒ½"""
        # PostgreSQLæŸ¥è¯¢è®¡åˆ’åˆ†æ
        explain_sql = f"EXPLAIN ANALYZE {sql}"
        result = self.session.execute(text(explain_sql), params or {})
        
        execution_plan = []
        for row in result:
            execution_plan.append(row[0])
        
        return execution_plan
    
    def suggest_indexes(self, table_name: str, query_patterns: List[Dict[str, Any]]):
        """å»ºè®®ç´¢å¼•"""
        suggestions = []
        
        for pattern in query_patterns:
            where_columns = pattern.get('where_columns', [])
            order_columns = pattern.get('order_columns', [])
            join_columns = pattern.get('join_columns', [])
            
            # å»ºè®®å¤åˆç´¢å¼•
            if len(where_columns) > 1:
                suggestions.append({
                    'type': 'composite_index',
                    'columns': where_columns,
                    'reason': 'Multiple WHERE conditions'
                })
            
            # å»ºè®®æ’åºç´¢å¼•
            if order_columns:
                suggestions.append({
                    'type': 'order_index',
                    'columns': order_columns,
                    'reason': 'ORDER BY optimization'
                })
            
            # å»ºè®®è¿æ¥ç´¢å¼•
            if join_columns:
                suggestions.append({
                    'type': 'join_index',
                    'columns': join_columns,
                    'reason': 'JOIN optimization'
                })
        
        return suggestions
    
    def create_optimized_indexes(self, model, index_definitions: List[Dict[str, Any]]):
        """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
        for index_def in index_definitions:
            columns = [getattr(model, col) for col in index_def['columns']]
            
            index = Index(
                f"idx_{model.__tablename__}_{'_'.join(index_def['columns'])}",
                *columns,
                unique=index_def.get('unique', False)
            )
            
            # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåº”è¯¥é€šè¿‡è¿ç§»è„šæœ¬åˆ›å»ºç´¢å¼•
            # è¿™é‡Œåªæ˜¯ç¤ºä¾‹
            print(f"å»ºè®®åˆ›å»ºç´¢å¼•: {index}")

# æ•°æ®åº“ç›‘æ§
class DatabaseMonitoring:
    """æ•°æ®åº“æ€§èƒ½ç›‘æ§"""
    
    def __init__(self, session):
        self.session = session
    
    def get_slow_queries(self, threshold_ms: int = 1000):
        """è·å–æ…¢æŸ¥è¯¢"""
        # PostgreSQLæ…¢æŸ¥è¯¢ç›‘æ§
        sql = """
        SELECT query, mean_time, calls, total_time
        FROM pg_stat_statements
        WHERE mean_time > :threshold
        ORDER BY mean_time DESC
        LIMIT 20
        """
        
        result = self.session.execute(text(sql), {'threshold': threshold_ms})
        return result.fetchall()
    
    def get_table_stats(self, table_name: str):
        """è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯"""
        sql = """
        SELECT 
            schemaname,
            tablename,
            n_tup_ins as inserts,
            n_tup_upd as updates,
            n_tup_del as deletes,
            n_live_tup as live_tuples,
            n_dead_tup as dead_tuples,
            last_vacuum,
            last_autovacuum,
            last_analyze,
            last_autoanalyze
        FROM pg_stat_user_tables
        WHERE tablename = :table_name
        """
        
        result = self.session.execute(text(sql), {'table_name': table_name})
        return result.fetchone()
    
    def get_index_usage(self, table_name: str):
        """è·å–ç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
        sql = """
        SELECT 
            indexrelname as index_name,
            idx_tup_read as index_reads,
            idx_tup_fetch as index_fetches,
            idx_scan as index_scans
        FROM pg_stat_user_indexes
        WHERE relname = :table_name
        ORDER BY idx_scan DESC
        """
        
        result = self.session.execute(text(sql), {'table_name': table_name})
        return result.fetchall()
```

## ğŸš€ ç¼“å­˜ç­–ç•¥

### 1. å¤šå±‚ç¼“å­˜æ¶æ„

```python
import asyncio
import json
import time
from typing import Any, Optional, Dict, Union
from abc import ABC, abstractmethod
from enum import Enum

class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«"""
    L1_MEMORY = "l1_memory"      # å†…å­˜ç¼“å­˜
    L2_REDIS = "l2_redis"        # Redisç¼“å­˜
    L3_DATABASE = "l3_database"  # æ•°æ®åº“ç¼“å­˜

class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    WRITE_THROUGH = "write_through"    # å†™ç©¿é€
    WRITE_BACK = "write_back"          # å†™å›
    WRITE_AROUND = "write_around"      # å†™ç»•è¿‡

class CacheBackend(ABC):
    """ç¼“å­˜åç«¯æŠ½è±¡ç±»"""
    
    @abstractmethod
    async def get(self, key: str) -> Optional[Any]:
        pass
    
    @abstractmethod
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        pass
    
    @abstractmethod
    async def delete(self, key: str) -> bool:
        pass
    
    @abstractmethod
    async def exists(self, key: str) -> bool:
        pass

class MemoryCache(CacheBackend):
    """å†…å­˜ç¼“å­˜å®ç°"""
    
    def __init__(self, max_size: int = 1000):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.max_size = max_size
        self.access_times: Dict[str, float] = {}
    
    async def get(self, key: str) -> Optional[Any]:
        if key in self.cache:
            item = self.cache[key]
            if item['expires_at'] is None or item['expires_at'] > time.time():
                self.access_times[key] = time.time()
                return item['value']
            else:
                await self.delete(key)
        return None
    
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        # LRUæ·˜æ±°ç­–ç•¥
        if len(self.cache) >= self.max_size and key not in self.cache:
            lru_key = min(self.access_times.keys(), key=self.access_times.get)
            await self.delete(lru_key)
        
        expires_at = None
        if ttl:
            expires_at = time.time() + ttl
        
        self.cache[key] = {
            'value': value,
            'expires_at': expires_at
        }
        self.access_times[key] = time.time()
        return True
    
    async def delete(self, key: str) -> bool:
        if key in self.cache:
            del self.cache[key]
            del self.access_times[key]
            return True
        return False
    
    async def exists(self, key: str) -> bool:
        return key in self.cache

class RedisCache(CacheBackend):
    """Redisç¼“å­˜å®ç°"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def get(self, key: str) -> Optional[Any]:
        value = await self.redis.get(key)
        if value:
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return value.decode() if isinstance(value, bytes) else value
        return None
    
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        try:
            serialized_value = json.dumps(value)
        except (TypeError, ValueError):
            serialized_value = str(value)
        
        if ttl:
            return await self.redis.setex(key, ttl, serialized_value)
        else:
            return await self.redis.set(key, serialized_value)
    
    async def delete(self, key: str) -> bool:
        return await self.redis.delete(key) > 0
    
    async def exists(self, key: str) -> bool:
        return await self.redis.exists(key) > 0

class MultiLevelCache:
    """å¤šå±‚ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.backends: Dict[CacheLevel, CacheBackend] = {}
        self.strategy = CacheStrategy.WRITE_THROUGH
    
    def add_backend(self, level: CacheLevel, backend: CacheBackend):
        """æ·»åŠ ç¼“å­˜åç«¯"""
        self.backends[level] = backend
    
    async def get(self, key: str) -> Optional[Any]:
        """å¤šå±‚ç¼“å­˜è·å–"""
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæŸ¥æ‰¾
        for level in [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS]:
            if level in self.backends:
                value = await self.backends[level].get(key)
                if value is not None:
                    # å›å¡«åˆ°æ›´é«˜çº§åˆ«çš„ç¼“å­˜
                    await self._backfill_cache(key, value, level)
                    return value
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """å¤šå±‚ç¼“å­˜è®¾ç½®"""
        success = True
        
        if self.strategy == CacheStrategy.WRITE_THROUGH:
            # å†™ç©¿é€ï¼šåŒæ—¶å†™å…¥æ‰€æœ‰å±‚çº§
            for backend in self.backends.values():
                result = await backend.set(key, value, ttl)
                success = success and result
        
        elif self.strategy == CacheStrategy.WRITE_BACK:
            # å†™å›ï¼šåªå†™å…¥æœ€é«˜çº§åˆ«ç¼“å­˜
            if CacheLevel.L1_MEMORY in self.backends:
                success = await self.backends[CacheLevel.L1_MEMORY].set(key, value, ttl)
        
        return success
    
    async def delete(self, key: str) -> bool:
        """åˆ é™¤æ‰€æœ‰å±‚çº§çš„ç¼“å­˜"""
        success = True
        for backend in self.backends.values():
            result = await backend.delete(key)
            success = success and result
        return success
    
    async def _backfill_cache(self, key: str, value: Any, found_level: CacheLevel):
        """å›å¡«ç¼“å­˜åˆ°æ›´é«˜çº§åˆ«"""
        levels = [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS, CacheLevel.L3_DATABASE]
        found_index = levels.index(found_level)
        
        # å›å¡«åˆ°æ›´é«˜çº§åˆ«çš„ç¼“å­˜
        for i in range(found_index):
            level = levels[i]
            if level in self.backends:
                await self.backends[level].set(key, value)

# ç¼“å­˜è£…é¥°å™¨
class CacheDecorator:
    """ç¼“å­˜è£…é¥°å™¨"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
    
    def cached(self, ttl: int = 300, key_prefix: str = ""):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @functools.wraps(func)
            async def async_wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_cache_key(func, args, kwargs, key_prefix)
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = await self.cache.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)
                
                # ç¼“å­˜ç»“æœ
                await self.cache.set(cache_key, result, ttl)
                
                return result
            
            @functools.wraps(func)
            def sync_wrapper(*args, **kwargs):
                # åŒæ­¥ç‰ˆæœ¬çš„å®ç°
                cache_key = self._generate_cache_key(func, args, kwargs, key_prefix)
                
                # è¿™é‡Œéœ€è¦åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
                loop = asyncio.get_event_loop()
                
                cached_result = loop.run_until_complete(self.cache.get(cache_key))
                if cached_result is not None:
                    return cached_result
                
                result = func(*args, **kwargs)
                loop.run_until_complete(self.cache.set(cache_key, result, ttl))
                
                return result
            
            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        return decorator
    
    def _generate_cache_key(self, func, args, kwargs, prefix: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        
        key_data = {
            'function': func.__name__,
            'args': args,
            'kwargs': kwargs
        }
        
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        key_hash = hashlib.md5(key_str.encode()).hexdigest()
        
        return f"{prefix}:{func.__name__}:{key_hash}" if prefix else f"{func.__name__}:{key_hash}"

# ä½¿ç”¨ç¤ºä¾‹
async def setup_cache_system():
    # åˆ›å»ºå¤šå±‚ç¼“å­˜
    cache = MultiLevelCache()
    
    # æ·»åŠ å†…å­˜ç¼“å­˜
    memory_cache = MemoryCache(max_size=1000)
    cache.add_backend(CacheLevel.L1_MEMORY, memory_cache)
    
    # æ·»åŠ Redisç¼“å­˜
    import aioredis
    redis_client = aioredis.from_url("redis://localhost")
    redis_cache = RedisCache(redis_client)
    cache.add_backend(CacheLevel.L2_REDIS, redis_cache)
    
    # åˆ›å»ºç¼“å­˜è£…é¥°å™¨
    cache_decorator = CacheDecorator(cache)
    
    # ä½¿ç”¨ç¼“å­˜è£…é¥°å™¨
    @cache_decorator.cached(ttl=600, key_prefix="user_data")
    async def get_user_data(user_id: int):
        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        await asyncio.sleep(0.1)
        return {"user_id": user_id, "name": f"User {user_id}"}
    
    # æµ‹è¯•ç¼“å­˜
    result1 = await get_user_data(123)  # ä»æ•°æ®åº“è·å–
    result2 = await get_user_data(123)  # ä»ç¼“å­˜è·å–
    
    return cache
```

### 2. ç¼“å­˜å¤±æ•ˆç­–ç•¥

```python
import asyncio
import time
from typing import Set, Dict, List, Callable, Any
from dataclasses import dataclass
from enum import Enum

class InvalidationStrategy(Enum):
    """ç¼“å­˜å¤±æ•ˆç­–ç•¥"""
    TTL = "ttl"                    # åŸºäºæ—¶é—´çš„å¤±æ•ˆ
    TAG_BASED = "tag_based"        # åŸºäºæ ‡ç­¾çš„å¤±æ•ˆ
    DEPENDENCY = "dependency"      # åŸºäºä¾èµ–çš„å¤±æ•ˆ
    MANUAL = "manual"              # æ‰‹åŠ¨å¤±æ•ˆ

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: Any
    created_at: float
    ttl: int
    tags: Set[str]
    dependencies: Set[str]
    access_count: int = 0
    last_access: float = 0

class CacheInvalidationManager:
    """ç¼“å­˜å¤±æ•ˆç®¡ç†å™¨"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.entries: Dict[str, CacheEntry] = {}
        self.tag_mapping: Dict[str, Set[str]] = {}  # tag -> keys
        self.dependency_mapping: Dict[str, Set[str]] = {}  # dependency -> keys
        self.invalidation_callbacks: List[Callable] = []
    
    async def set_with_metadata(
        self, 
        key: str, 
        value: Any, 
        ttl: int = None,
        tags: Set[str] = None,
        dependencies: Set[str] = None
    ):
        """è®¾ç½®ç¼“å­˜å¹¶è®°å½•å…ƒæ•°æ®"""
        tags = tags or set()
        dependencies = dependencies or set()
        
        # åˆ›å»ºç¼“å­˜æ¡ç›®
        entry = CacheEntry(
            key=key,
            value=value,
            created_at=time.time(),
            ttl=ttl,
            tags=tags,
            dependencies=dependencies
        )
        
        # å­˜å‚¨åˆ°ç¼“å­˜
        await self.cache.set(key, value, ttl)
        
        # è®°å½•å…ƒæ•°æ®
        self.entries[key] = entry
        
        # æ›´æ–°æ ‡ç­¾æ˜ å°„
        for tag in tags:
            if tag not in self.tag_mapping:
                self.tag_mapping[tag] = set()
            self.tag_mapping[tag].add(key)
        
        # æ›´æ–°ä¾èµ–æ˜ å°„
        for dep in dependencies:
            if dep not in self.dependency_mapping:
                self.dependency_mapping[dep] = set()
            self.dependency_mapping[dep].add(key)
    
    async def invalidate_by_tag(self, tag: str):
        """æ ¹æ®æ ‡ç­¾å¤±æ•ˆç¼“å­˜"""
        if tag in self.tag_mapping:
            keys_to_invalidate = self.tag_mapping[tag].copy()
            
            for key in keys_to_invalidate:
                await self._invalidate_key(key)
            
            # æ¸…ç†æ ‡ç­¾æ˜ å°„
            del self.tag_mapping[tag]
    
    async def invalidate_by_dependency(self, dependency: str):
        """æ ¹æ®ä¾èµ–å¤±æ•ˆç¼“å­˜"""
        if dependency in self.dependency_mapping:
            keys_to_invalidate = self.dependency_mapping[dependency].copy()
            
            for key in keys_to_invalidate:
                await self._invalidate_key(key)
            
            # æ¸…ç†ä¾èµ–æ˜ å°„
            del self.dependency_mapping[dependency]
    
    async def invalidate_expired(self):
        """å¤±æ•ˆè¿‡æœŸçš„ç¼“å­˜"""
        current_time = time.time()
        expired_keys = []
        
        for key, entry in self.entries.items():
            if entry.ttl and (entry.created_at + entry.ttl) < current_time:
                expired_keys.append(key)
        
        for key in expired_keys:
            await self._invalidate_key(key)
    
    async def _invalidate_key(self, key: str):
        """å¤±æ•ˆå•ä¸ªç¼“å­˜é”®"""
        # ä»ç¼“å­˜ä¸­åˆ é™¤
        await self.cache.delete(key)
        
        # æ¸…ç†å…ƒæ•°æ®
        if key in self.entries:
            entry = self.entries[key]
            
            # æ¸…ç†æ ‡ç­¾æ˜ å°„
            for tag in entry.tags:
                if tag in self.tag_mapping:
                    self.tag_mapping[tag].discard(key)
                    if not self.tag_mapping[tag]:
                        del self.tag_mapping[tag]
            
            # æ¸…ç†ä¾èµ–æ˜ å°„
            for dep in entry.dependencies:
                if dep in self.dependency_mapping:
                    self.dependency_mapping[dep].discard(key)
                    if not self.dependency_mapping[dep]:
                        del self.dependency_mapping[dep]
            
            del self.entries[key]
        
        # è°ƒç”¨å¤±æ•ˆå›è°ƒ
        for callback in self.invalidation_callbacks:
            try:
                await callback(key)
            except Exception as e:
                print(f"Invalidation callback error: {e}")
    
    def add_invalidation_callback(self, callback: Callable):
        """æ·»åŠ å¤±æ•ˆå›è°ƒ"""
        self.invalidation_callbacks.append(callback)
    
    async def start_cleanup_task(self, interval: int = 60):
        """å¯åŠ¨æ¸…ç†ä»»åŠ¡"""
        while True:
            try:
                await self.invalidate_expired()
                await asyncio.sleep(interval)
            except Exception as e:
                print(f"Cache cleanup error: {e}")
                await asyncio.sleep(interval)

# æ™ºèƒ½ç¼“å­˜é¢„çƒ­
class CacheWarmup:
    """ç¼“å­˜é¢„çƒ­ç®¡ç†å™¨"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.warmup_tasks: List[Dict[str, Any]] = []
    
    def add_warmup_task(
        self, 
        func: Callable, 
        args: tuple = (), 
        kwargs: dict = None,
        priority: int = 1,
        schedule: str = "startup"  # startup, periodic, on_demand
    ):
        """æ·»åŠ é¢„çƒ­ä»»åŠ¡"""
        self.warmup_tasks.append({
            'func': func,
            'args': args,
            'kwargs': kwargs or {},
            'priority': priority,
            'schedule': schedule
        })
    
    async def execute_warmup(self, schedule_type: str = "startup"):
        """æ‰§è¡Œé¢„çƒ­ä»»åŠ¡"""
        tasks = [
            task for task in self.warmup_tasks 
            if task['schedule'] == schedule_type
        ]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        tasks.sort(key=lambda x: x['priority'], reverse=True)
        
        for task in tasks:
            try:
                func = task['func']
                args = task['args']
                kwargs = task['kwargs']
                
                if asyncio.iscoroutinefunction(func):
                    await func(*args, **kwargs)
                else:
                    func(*args, **kwargs)
                    
            except Exception as e:
                print(f"Warmup task error: {e}")
    
    async def intelligent_warmup(self, access_patterns: Dict[str, int]):
        """åŸºäºè®¿é—®æ¨¡å¼çš„æ™ºèƒ½é¢„çƒ­"""
        # æ ¹æ®è®¿é—®é¢‘ç‡é¢„çƒ­çƒ­ç‚¹æ•°æ®
        sorted_patterns = sorted(
            access_patterns.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        for cache_key, access_count in sorted_patterns[:10]:  # é¢„çƒ­å‰10ä¸ªçƒ­ç‚¹
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ä¸šåŠ¡é€»è¾‘æ¥é‡æ–°ç”Ÿæˆæ•°æ®
            print(f"Warming up cache key: {cache_key} (accessed {access_count} times)")

# ä½¿ç”¨ç¤ºä¾‹
async def cache_invalidation_example():
    # è®¾ç½®ç¼“å­˜ç³»ç»Ÿ
    cache = MultiLevelCache()
    memory_cache = MemoryCache()
    cache.add_backend(CacheLevel.L1_MEMORY, memory_cache)
    
    # åˆ›å»ºå¤±æ•ˆç®¡ç†å™¨
    invalidation_manager = CacheInvalidationManager(cache)
    
    # è®¾ç½®å¸¦æ ‡ç­¾çš„ç¼“å­˜
    await invalidation_manager.set_with_metadata(
        "user:123",
        {"name": "John", "email": "john@example.com"},
        ttl=3600,
        tags={"user", "profile"},
        dependencies={"user_table"}
    )
    
    # æ ¹æ®æ ‡ç­¾å¤±æ•ˆ
    await invalidation_manager.invalidate_by_tag("user")
    
    # æ ¹æ®ä¾èµ–å¤±æ•ˆ
    await invalidation_manager.invalidate_by_dependency("user_table")
    
    # å¯åŠ¨æ¸…ç†ä»»åŠ¡
    asyncio.create_task(invalidation_manager.start_cleanup_task())
```