# ç›‘æ§æ—¥å¿—æ¨¡å—ä½¿ç”¨æŒ‡å—

ç›‘æ§æ—¥å¿—æ¨¡å—æä¾›äº†å®Œæ•´çš„åº”ç”¨ç›‘æ§ã€æ—¥å¿—ç®¡ç†ã€å¥åº·æ£€æŸ¥ã€é”™è¯¯è¿½è¸ªå’Œæ€§èƒ½åˆ†æåŠŸèƒ½ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [åŸºç¡€ä½¿ç”¨](#åŸºç¡€ä½¿ç”¨)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install structlog prometheus-client sentry-sdk psutil
```

### åŸºç¡€é…ç½®

```python
from templates.monitoring import LoggingConfig, StructuredLogger, HealthCheckManager

# åˆ›å»ºæ—¥å¿—é…ç½®
logging_config = LoggingConfig(
    LOG_LEVEL="INFO",
    LOG_FORMAT="json",
    LOG_FILE="app.log",
    ENABLE_CONSOLE_LOGGING=True,
    ENABLE_FILE_LOGGING=True
)

# åˆ›å»ºç»“æ„åŒ–æ—¥å¿—å™¨
logger = StructuredLogger(logging_config)

# åˆ›å»ºå¥åº·æ£€æŸ¥ç®¡ç†å™¨
health_manager = HealthCheckManager()
```

## âš™ï¸ é…ç½®è¯´æ˜

### LoggingConfig å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `LOG_LEVEL` | str | "INFO" | æ—¥å¿—çº§åˆ« |
| `LOG_FORMAT` | str | "json" | æ—¥å¿—æ ¼å¼(json/text) |
| `LOG_FILE` | str | "app.log" | æ—¥å¿—æ–‡ä»¶è·¯å¾„ |
| `LOG_MAX_SIZE` | int | 10485760 | æ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°(å­—èŠ‚) |
| `LOG_BACKUP_COUNT` | int | 5 | æ—¥å¿—æ–‡ä»¶å¤‡ä»½æ•°é‡ |
| `ENABLE_CONSOLE_LOGGING` | bool | True | å¯ç”¨æ§åˆ¶å°æ—¥å¿— |
| `ENABLE_FILE_LOGGING` | bool | True | å¯ç”¨æ–‡ä»¶æ—¥å¿— |
| `ENABLE_SYSLOG` | bool | False | å¯ç”¨ç³»ç»Ÿæ—¥å¿— |
| `SYSLOG_HOST` | str | "localhost" | ç³»ç»Ÿæ—¥å¿—ä¸»æœº |
| `SYSLOG_PORT` | int | 514 | ç³»ç»Ÿæ—¥å¿—ç«¯å£ |
| `LOG_CORRELATION_ID` | bool | True | å¯ç”¨å…³è”ID |

### ç›‘æ§é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `METRICS_ENABLED` | bool | True | å¯ç”¨æŒ‡æ ‡æ”¶é›† |
| `METRICS_PORT` | int | 8000 | æŒ‡æ ‡æœåŠ¡ç«¯å£ |
| `HEALTH_CHECK_INTERVAL` | int | 30 | å¥åº·æ£€æŸ¥é—´éš”(ç§’) |
| `ERROR_TRACKING_ENABLED` | bool | True | å¯ç”¨é”™è¯¯è¿½è¸ª |
| `SENTRY_DSN` | str | None | Sentry DSN |
| `PERFORMANCE_MONITORING` | bool | True | å¯ç”¨æ€§èƒ½ç›‘æ§ |
| `TRACE_SAMPLING_RATE` | float | 0.1 | è¿½è¸ªé‡‡æ ·ç‡ |

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_FILE=logs/app.log
ENABLE_CONSOLE_LOGGING=true
ENABLE_FILE_LOGGING=true

METRICS_ENABLED=true
METRICS_PORT=8000
HEALTH_CHECK_INTERVAL=30
ERROR_TRACKING_ENABLED=true
SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id
PERFORMANCE_MONITORING=true
TRACE_SAMPLING_RATE=0.1
```

## ğŸ’» åŸºç¡€ä½¿ç”¨

### 1. ç»“æ„åŒ–æ—¥å¿—

```python
from templates.monitoring import StructuredLogger
import structlog

# åˆ›å»ºæ—¥å¿—å™¨
logger = StructuredLogger()

# åŸºç¡€æ—¥å¿—è®°å½•
logger.info("User login", user_id=123, username="john_doe")
logger.warning("High memory usage", memory_percent=85.2)
logger.error("Database connection failed", error="Connection timeout", retry_count=3)

# å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—
with logger.bind(request_id="req-123", user_id=456):
    logger.info("Processing request")
    logger.debug("Validating input data")
    logger.info("Request completed", duration_ms=250)

# å¼‚å¸¸æ—¥å¿—
try:
    result = risky_operation()
except Exception as e:
    logger.exception("Operation failed", operation="risky_operation", input_data=data)

# æ€§èƒ½æ—¥å¿—
with logger.timer("database_query"):
    users = db.query(User).all()
    logger.info("Query completed", result_count=len(users))
```

### 2. å¥åº·æ£€æŸ¥

```python
from templates.monitoring import HealthCheck, HealthCheckManager
from fastapi import APIRouter

# å®šä¹‰å¥åº·æ£€æŸ¥
class DatabaseHealthCheck(HealthCheck):
    def __init__(self, db_manager):
        self.db_manager = db_manager
        super().__init__(name="database", timeout=5)
    
    async def check(self) -> dict:
        try:
            with self.db_manager.get_session() as session:
                session.execute("SELECT 1")
            return {
                "status": "healthy",
                "message": "Database connection successful"
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "message": f"Database connection failed: {str(e)}"
            }

class RedisHealthCheck(HealthCheck):
    def __init__(self, redis_manager):
        self.redis_manager = redis_manager
        super().__init__(name="redis", timeout=3)
    
    async def check(self) -> dict:
        try:
            await self.redis_manager.ping()
            return {
                "status": "healthy",
                "message": "Redis connection successful"
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "message": f"Redis connection failed: {str(e)}"
            }

# æ³¨å†Œå¥åº·æ£€æŸ¥
health_manager.register_check(DatabaseHealthCheck(db_manager))
health_manager.register_check(RedisHealthCheck(redis_manager))

# åˆ›å»ºå¥åº·æ£€æŸ¥ç«¯ç‚¹
router = APIRouter()

@router.get("/health")
async def health_check():
    results = await health_manager.check_all()
    
    overall_status = "healthy" if all(
        check["status"] == "healthy" for check in results.values()
    ) else "unhealthy"
    
    return {
        "status": overall_status,
        "timestamp": datetime.utcnow().isoformat(),
        "checks": results
    }

@router.get("/health/{service}")
async def service_health_check(service: str):
    result = await health_manager.check_service(service)
    if result is None:
        raise HTTPException(status_code=404, detail="Service not found")
    
    return result
```

### 3. æŒ‡æ ‡æ”¶é›†

```python
from templates.monitoring import MetricsCollector, PrometheusMetrics
from prometheus_client import Counter, Histogram, Gauge

# åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨
metrics = PrometheusMetrics()

# å®šä¹‰æŒ‡æ ‡
request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

active_users = Gauge(
    'active_users_total',
    'Number of active users'
)

database_connections = Gauge(
    'database_connections_active',
    'Active database connections'
)

# åœ¨APIä¸­ä½¿ç”¨æŒ‡æ ‡
from fastapi import Request, Response
import time

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    # è®°å½•è¯·æ±‚æŒ‡æ ‡
    duration = time.time() - start_time
    
    request_count.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    request_duration.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)
    
    return response

# ä¸šåŠ¡æŒ‡æ ‡
@router.post("/users")
async def create_user(user_data: UserCreate):
    user = await user_service.create_user(user_data)
    
    # æ›´æ–°æ´»è·ƒç”¨æˆ·æ•°
    active_count = await user_service.get_active_user_count()
    active_users.set(active_count)
    
    return user

# ç³»ç»ŸæŒ‡æ ‡
import psutil

def collect_system_metrics():
    # CPUä½¿ç”¨ç‡
    cpu_percent = psutil.cpu_percent()
    metrics.set_gauge('system_cpu_percent', cpu_percent)
    
    # å†…å­˜ä½¿ç”¨ç‡
    memory = psutil.virtual_memory()
    metrics.set_gauge('system_memory_percent', memory.percent)
    metrics.set_gauge('system_memory_used_bytes', memory.used)
    
    # ç£ç›˜ä½¿ç”¨ç‡
    disk = psutil.disk_usage('/')
    metrics.set_gauge('system_disk_percent', disk.percent)
    
    # ç½‘ç»œIO
    network = psutil.net_io_counters()
    metrics.set_counter('system_network_bytes_sent', network.bytes_sent)
    metrics.set_counter('system_network_bytes_recv', network.bytes_recv)
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. åˆ†å¸ƒå¼è¿½è¸ª

```python
from templates.monitoring import TracingManager, TraceContext
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter

# é…ç½®è¿½è¸ª
tracing_manager = TracingManager(
    service_name="my-api",
    jaeger_endpoint="http://localhost:14268/api/traces",
    sampling_rate=0.1
)

# æ‰‹åŠ¨åˆ›å»ºspan
async def process_order(order_id: int):
    tracer = trace.get_tracer(__name__)
    
    with tracer.start_as_current_span("process_order") as span:
        span.set_attribute("order.id", order_id)
        
        # éªŒè¯è®¢å•
        with tracer.start_as_current_span("validate_order") as validate_span:
            is_valid = await validate_order(order_id)
            validate_span.set_attribute("order.valid", is_valid)
            
            if not is_valid:
                span.set_status(trace.Status(trace.StatusCode.ERROR, "Invalid order"))
                return False
        
        # å¤„ç†æ”¯ä»˜
        with tracer.start_as_current_span("process_payment") as payment_span:
            payment_result = await process_payment(order_id)
            payment_span.set_attribute("payment.success", payment_result.success)
            payment_span.set_attribute("payment.amount", payment_result.amount)
        
        # æ›´æ–°åº“å­˜
        with tracer.start_as_current_span("update_inventory") as inventory_span:
            await update_inventory(order_id)
        
        span.set_attribute("order.processed", True)
        return True

# è‡ªåŠ¨è¿½è¸ªè£…é¥°å™¨
from templates.monitoring import trace_function

@trace_function("user_service")
async def get_user_profile(user_id: int):
    # è‡ªåŠ¨åˆ›å»ºspanå¹¶è®°å½•å‚æ•°
    user = await db.get_user(user_id)
    return user

# è¿½è¸ªHTTPè¯·æ±‚
@app.middleware("http")
async def tracing_middleware(request: Request, call_next):
    tracer = trace.get_tracer(__name__)
    
    with tracer.start_as_current_span(
        f"{request.method} {request.url.path}"
    ) as span:
        span.set_attribute("http.method", request.method)
        span.set_attribute("http.url", str(request.url))
        span.set_attribute("http.user_agent", request.headers.get("user-agent", ""))
        
        response = await call_next(request)
        
        span.set_attribute("http.status_code", response.status_code)
        
        if response.status_code >= 400:
            span.set_status(trace.Status(trace.StatusCode.ERROR))
        
        return response
```

### 2. é”™è¯¯è¿½è¸ªå’Œå‘Šè­¦

```python
from templates.monitoring import ErrorTracker, AlertManager
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration

# é…ç½®Sentry
sentry_sdk.init(
    dsn="https://your-sentry-dsn@sentry.io/project-id",
    integrations=[
        FastApiIntegration(auto_enabling_integrations=False),
        SqlalchemyIntegration(),
    ],
    traces_sample_rate=0.1,
    environment="production"
)

# é”™è¯¯è¿½è¸ªå™¨
class CustomErrorTracker(ErrorTracker):
    def __init__(self):
        self.error_counts = {}
        self.alert_manager = AlertManager()
    
    async def track_error(self, error: Exception, context: dict = None):
        error_type = type(error).__name__
        
        # è®°å½•é”™è¯¯
        logger.error(
            "Error occurred",
            error_type=error_type,
            error_message=str(error),
            context=context or {},
            exc_info=error
        )
        
        # å‘é€åˆ°Sentry
        with sentry_sdk.push_scope() as scope:
            if context:
                for key, value in context.items():
                    scope.set_tag(key, value)
            sentry_sdk.capture_exception(error)
        
        # é”™è¯¯è®¡æ•°å’Œå‘Šè­¦
        self.error_counts[error_type] = self.error_counts.get(error_type, 0) + 1
        
        if self.error_counts[error_type] > 10:  # é”™è¯¯æ¬¡æ•°è¶…è¿‡é˜ˆå€¼
            await self.alert_manager.send_alert(
                level="critical",
                message=f"High error rate for {error_type}: {self.error_counts[error_type]} occurrences",
                context=context
            )

error_tracker = CustomErrorTracker()

# å…¨å±€å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    await error_tracker.track_error(exc, {
        "request_path": request.url.path,
        "request_method": request.method,
        "user_agent": request.headers.get("user-agent")
    })
    
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error", "request_id": request.state.request_id}
    )

# ä¸šåŠ¡é”™è¯¯è¿½è¸ª
async def process_payment(order_id: int, amount: float):
    try:
        result = await payment_service.charge(order_id, amount)
        return result
    except PaymentError as e:
        await error_tracker.track_error(e, {
            "order_id": order_id,
            "amount": amount,
            "payment_method": "credit_card"
        })
        raise
    except Exception as e:
        await error_tracker.track_error(e, {
            "order_id": order_id,
            "amount": amount,
            "operation": "payment_processing"
        })
        raise
```

### 3. æ€§èƒ½ç›‘æ§

```python
from templates.monitoring import PerformanceMonitor, ProfilerManager
import cProfile
import pstats
from io import StringIO

# æ€§èƒ½ç›‘æ§å™¨
class ApplicationPerformanceMonitor(PerformanceMonitor):
    def __init__(self):
        self.slow_queries = []
        self.slow_requests = []
        self.memory_usage = []
    
    async def monitor_database_query(self, query: str, duration: float):
        if duration > 1.0:  # æ…¢æŸ¥è¯¢é˜ˆå€¼1ç§’
            self.slow_queries.append({
                "query": query,
                "duration": duration,
                "timestamp": datetime.utcnow()
            })
            
            logger.warning(
                "Slow database query detected",
                query=query[:100],  # æˆªæ–­é•¿æŸ¥è¯¢
                duration=duration
            )
    
    async def monitor_request(self, path: str, method: str, duration: float):
        if duration > 2.0:  # æ…¢è¯·æ±‚é˜ˆå€¼2ç§’
            self.slow_requests.append({
                "path": path,
                "method": method,
                "duration": duration,
                "timestamp": datetime.utcnow()
            })
            
            logger.warning(
                "Slow request detected",
                path=path,
                method=method,
                duration=duration
            )
    
    def get_performance_report(self) -> dict:
        return {
            "slow_queries_count": len(self.slow_queries),
            "slow_requests_count": len(self.slow_requests),
            "avg_query_time": self._calculate_avg_query_time(),
            "avg_request_time": self._calculate_avg_request_time()
        }

perf_monitor = ApplicationPerformanceMonitor()

# æ•°æ®åº“æŸ¥è¯¢ç›‘æ§
class MonitoredDatabase:
    def __init__(self, db_manager, perf_monitor):
        self.db = db_manager
        self.perf_monitor = perf_monitor
    
    async def execute_query(self, query: str, params=None):
        start_time = time.time()
        try:
            with self.db.get_session() as session:
                result = session.execute(query, params)
                return result
        finally:
            duration = time.time() - start_time
            await self.perf_monitor.monitor_database_query(query, duration)

# ä»£ç æ€§èƒ½åˆ†æ
class CodeProfiler:
    def __init__(self):
        self.profiler = cProfile.Profile()
    
    def start_profiling(self):
        self.profiler.enable()
    
    def stop_profiling(self) -> str:
        self.profiler.disable()
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        s = StringIO()
        ps = pstats.Stats(self.profiler, stream=s)
        ps.sort_stats('cumulative')
        ps.print_stats(20)  # æ˜¾ç¤ºå‰20ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
        
        return s.getvalue()
    
    def profile_function(self, func):
        def wrapper(*args, **kwargs):
            self.start_profiling()
            try:
                return func(*args, **kwargs)
            finally:
                report = self.stop_profiling()
                logger.info("Function profiling report", 
                           function=func.__name__, 
                           report=report)
        return wrapper

profiler = CodeProfiler()

# ä½¿ç”¨æ€§èƒ½åˆ†æ
@profiler.profile_function
def expensive_computation(data):
    # è€—æ—¶è®¡ç®—
    result = complex_algorithm(data)
    return result
```

### 4. æ—¥å¿—èšåˆå’Œåˆ†æ

```python
from templates.monitoring import LogAggregator, LogAnalyzer
from elasticsearch import Elasticsearch

# æ—¥å¿—èšåˆå™¨
class ElasticsearchLogAggregator(LogAggregator):
    def __init__(self, es_hosts):
        self.es = Elasticsearch(es_hosts)
        self.index_name = "application-logs"
    
    async def send_log(self, log_entry: dict):
        try:
            await self.es.index(
                index=self.index_name,
                body=log_entry
            )
        except Exception as e:
            # é¿å…æ—¥å¿—å‘é€å¤±è´¥å½±å“ä¸»ä¸šåŠ¡
            print(f"Failed to send log to Elasticsearch: {e}")
    
    async def search_logs(self, query: dict, size: int = 100):
        try:
            response = await self.es.search(
                index=self.index_name,
                body=query,
                size=size
            )
            return response['hits']['hits']
        except Exception as e:
            logger.error("Failed to search logs", error=str(e))
            return []

# æ—¥å¿—åˆ†æå™¨
class LogAnalyzer:
    def __init__(self, log_aggregator):
        self.aggregator = log_aggregator
    
    async def analyze_error_patterns(self, time_range: str = "1h"):
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"level": "ERROR"}},
                        {"range": {
                            "timestamp": {
                                "gte": f"now-{time_range}"
                            }
                        }}
                    ]
                }
            },
            "aggs": {
                "error_types": {
                    "terms": {
                        "field": "error_type.keyword",
                        "size": 10
                    }
                },
                "error_timeline": {
                    "date_histogram": {
                        "field": "timestamp",
                        "interval": "5m"
                    }
                }
            }
        }
        
        results = await self.aggregator.search_logs(query)
        return self._process_error_analysis(results)
    
    async def analyze_performance_trends(self, endpoint: str = None):
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"exists": {"field": "duration_ms"}}
                    ]
                }
            },
            "aggs": {
                "avg_duration": {
                    "avg": {"field": "duration_ms"}
                },
                "max_duration": {
                    "max": {"field": "duration_ms"}
                },
                "duration_percentiles": {
                    "percentiles": {
                        "field": "duration_ms",
                        "percents": [50, 90, 95, 99]
                    }
                }
            }
        }
        
        if endpoint:
            query["query"]["bool"]["must"].append(
                {"term": {"endpoint.keyword": endpoint}}
            )
        
        results = await self.aggregator.search_logs(query)
        return self._process_performance_analysis(results)

log_aggregator = ElasticsearchLogAggregator(["localhost:9200"])
log_analyzer = LogAnalyzer(log_aggregator)
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ—¥å¿—ç»“æ„åŒ–

```python
# æ ‡å‡†åŒ–æ—¥å¿—å­—æ®µ
class LogFields:
    REQUEST_ID = "request_id"
    USER_ID = "user_id"
    SESSION_ID = "session_id"
    OPERATION = "operation"
    DURATION_MS = "duration_ms"
    ERROR_CODE = "error_code"
    ERROR_TYPE = "error_type"
    ENDPOINT = "endpoint"
    METHOD = "method"
    STATUS_CODE = "status_code"

# æ—¥å¿—ä¸Šä¸‹æ–‡ç®¡ç†
class LogContext:
    def __init__(self):
        self._context = {}
    
    def set(self, key: str, value):
        self._context[key] = value
    
    def get(self, key: str, default=None):
        return self._context.get(key, default)
    
    def clear(self):
        self._context.clear()
    
    def to_dict(self) -> dict:
        return self._context.copy()

# å…¨å±€æ—¥å¿—ä¸Šä¸‹æ–‡
log_context = LogContext()

# ä¸­é—´ä»¶è®¾ç½®ä¸Šä¸‹æ–‡
@app.middleware("http")
async def logging_context_middleware(request: Request, call_next):
    # ç”Ÿæˆè¯·æ±‚ID
    request_id = str(uuid.uuid4())
    log_context.set(LogFields.REQUEST_ID, request_id)
    log_context.set(LogFields.ENDPOINT, request.url.path)
    log_context.set(LogFields.METHOD, request.method)
    
    # ä»è¯·æ±‚å¤´è·å–ç”¨æˆ·ä¿¡æ¯
    user_id = request.headers.get("X-User-ID")
    if user_id:
        log_context.set(LogFields.USER_ID, user_id)
    
    try:
        response = await call_next(request)
        log_context.set(LogFields.STATUS_CODE, response.status_code)
        return response
    finally:
        log_context.clear()

# ä½¿ç”¨ä¸Šä¸‹æ–‡è®°å½•æ—¥å¿—
def log_with_context(level: str, message: str, **kwargs):
    context = log_context.to_dict()
    context.update(kwargs)
    
    getattr(logger, level)(message, **context)
```

### 2. ç›‘æ§å‘Šè­¦ç­–ç•¥

```python
# å‘Šè­¦è§„åˆ™å®šä¹‰
class AlertRule:
    def __init__(self, name: str, condition, threshold, duration: int = 300):
        self.name = name
        self.condition = condition
        self.threshold = threshold
        self.duration = duration  # æŒç»­æ—¶é—´(ç§’)
        self.triggered_at = None
    
    def check(self, value) -> bool:
        if self.condition(value, self.threshold):
            if self.triggered_at is None:
                self.triggered_at = time.time()
            elif time.time() - self.triggered_at >= self.duration:
                return True
        else:
            self.triggered_at = None
        
        return False

# å‘Šè­¦ç®¡ç†å™¨
class AlertManager:
    def __init__(self):
        self.rules = []
        self.alert_channels = []
    
    def add_rule(self, rule: AlertRule):
        self.rules.append(rule)
    
    def add_channel(self, channel):
        self.alert_channels.append(channel)
    
    async def check_alerts(self, metrics: dict):
        for rule in self.rules:
            metric_value = metrics.get(rule.name)
            if metric_value is not None and rule.check(metric_value):
                await self._send_alert(rule, metric_value)
    
    async def _send_alert(self, rule: AlertRule, value):
        alert_message = f"Alert: {rule.name} = {value} (threshold: {rule.threshold})"
        
        for channel in self.alert_channels:
            try:
                await channel.send(alert_message)
            except Exception as e:
                logger.error("Failed to send alert", channel=channel.__class__.__name__, error=str(e))

# å‘Šè­¦é€šé“
class SlackAlertChannel:
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
    
    async def send(self, message: str):
        payload = {
            "text": message,
            "username": "MonitorBot",
            "icon_emoji": ":warning:"
        }
        
        async with aiohttp.ClientSession() as session:
            await session.post(self.webhook_url, json=payload)

class EmailAlertChannel:
    def __init__(self, smtp_config: dict, recipients: list):
        self.smtp_config = smtp_config
        self.recipients = recipients
    
    async def send(self, message: str):
        # å®ç°é‚®ä»¶å‘é€é€»è¾‘
        pass

# é…ç½®å‘Šè­¦
alert_manager = AlertManager()

# æ·»åŠ å‘Šè­¦è§„åˆ™
alert_manager.add_rule(AlertRule(
    name="cpu_usage",
    condition=lambda x, threshold: x > threshold,
    threshold=80,
    duration=300  # 5åˆ†é’Ÿ
))

alert_manager.add_rule(AlertRule(
    name="error_rate",
    condition=lambda x, threshold: x > threshold,
    threshold=0.05,  # 5%é”™è¯¯ç‡
    duration=60  # 1åˆ†é’Ÿ
))

# æ·»åŠ å‘Šè­¦é€šé“
alert_manager.add_channel(SlackAlertChannel("https://hooks.slack.com/..."))
alert_manager.add_channel(EmailAlertChannel(smtp_config, ["admin@example.com"]))
```

### 3. æ€§èƒ½ä¼˜åŒ–

```python
# å¼‚æ­¥æ—¥å¿—å†™å…¥
import asyncio
from queue import Queue
from threading import Thread

class AsyncLogWriter:
    def __init__(self, log_file: str, batch_size: int = 100):
        self.log_file = log_file
        self.batch_size = batch_size
        self.log_queue = Queue()
        self.writer_thread = Thread(target=self._write_logs, daemon=True)
        self.writer_thread.start()
    
    def write_log(self, log_entry: dict):
        self.log_queue.put(log_entry)
    
    def _write_logs(self):
        batch = []
        
        while True:
            try:
                log_entry = self.log_queue.get(timeout=1)
                batch.append(log_entry)
                
                if len(batch) >= self.batch_size:
                    self._flush_batch(batch)
                    batch = []
                    
            except:
                if batch:
                    self._flush_batch(batch)
                    batch = []
    
    def _flush_batch(self, batch: list):
        with open(self.log_file, 'a') as f:
            for entry in batch:
                f.write(json.dumps(entry) + '\n')

# é‡‡æ ·æ—¥å¿—
class SamplingLogger:
    def __init__(self, logger, sample_rate: float = 0.1):
        self.logger = logger
        self.sample_rate = sample_rate
    
    def should_log(self) -> bool:
        return random.random() < self.sample_rate
    
    def debug(self, message: str, **kwargs):
        if self.should_log():
            self.logger.debug(message, **kwargs)
    
    def info(self, message: str, **kwargs):
        # INFOçº§åˆ«æ€»æ˜¯è®°å½•
        self.logger.info(message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        # WARNINGçº§åˆ«æ€»æ˜¯è®°å½•
        self.logger.warning(message, **kwargs)
    
    def error(self, message: str, **kwargs):
        # ERRORçº§åˆ«æ€»æ˜¯è®°å½•
        self.logger.error(message, **kwargs)

# æ¡ä»¶æ—¥å¿—
class ConditionalLogger:
    def __init__(self, logger):
        self.logger = logger
        self.conditions = {}
    
    def add_condition(self, level: str, condition):
        self.conditions[level] = condition
    
    def log(self, level: str, message: str, **kwargs):
        condition = self.conditions.get(level)
        
        if condition is None or condition(kwargs):
            getattr(self.logger, level)(message, **kwargs)

# ä½¿ç”¨æ¡ä»¶æ—¥å¿—
conditional_logger = ConditionalLogger(logger)

# åªè®°å½•æ…¢æŸ¥è¯¢
conditional_logger.add_condition(
    'debug',
    lambda kwargs: kwargs.get('duration_ms', 0) > 100
)

# åªè®°å½•ç‰¹å®šç”¨æˆ·çš„æ—¥å¿—
conditional_logger.add_condition(
    'info',
    lambda kwargs: kwargs.get('user_id') in ['admin', 'test_user']
)
```

## â“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•å¤„ç†å¤§é‡æ—¥å¿—æ•°æ®ï¼Ÿ

A: ä½¿ç”¨æ—¥å¿—è½®è½¬ã€å‹ç¼©å’Œå½’æ¡£ï¼š

```python
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
import gzip
import os

# æŒ‰å¤§å°è½®è½¬
rotating_handler = RotatingFileHandler(
    'app.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)

# æŒ‰æ—¶é—´è½®è½¬
timed_handler = TimedRotatingFileHandler(
    'app.log',
    when='midnight',
    interval=1,
    backupCount=30
)

# è‡ªåŠ¨å‹ç¼©æ—§æ—¥å¿—
def compress_old_logs(log_dir: str):
    for filename in os.listdir(log_dir):
        if filename.endswith('.log') and filename != 'app.log':
            filepath = os.path.join(log_dir, filename)
            
            with open(filepath, 'rb') as f_in:
                with gzip.open(f'{filepath}.gz', 'wb') as f_out:
                    f_out.writelines(f_in)
            
            os.remove(filepath)
```

### Q: å¦‚ä½•ç›‘æ§å¾®æœåŠ¡æ¶æ„ï¼Ÿ

A: ä½¿ç”¨åˆ†å¸ƒå¼è¿½è¸ªå’ŒæœåŠ¡ç½‘æ ¼ï¼š

```python
# æœåŠ¡é—´è°ƒç”¨è¿½è¸ª
class ServiceTracer:
    def __init__(self, service_name: str):
        self.service_name = service_name
    
    async def call_service(self, service_url: str, data: dict, trace_id: str = None):
        if trace_id is None:
            trace_id = str(uuid.uuid4())
        
        headers = {
            'X-Trace-ID': trace_id,
            'X-Service-Name': self.service_name
        }
        
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(service_url, json=data, headers=headers) as response:
                    duration = time.time() - start_time
                    
                    logger.info(
                        "Service call completed",
                        trace_id=trace_id,
                        target_service=service_url,
                        duration_ms=duration * 1000,
                        status_code=response.status
                    )
                    
                    return await response.json()
        
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "Service call failed",
                trace_id=trace_id,
                target_service=service_url,
                duration_ms=duration * 1000,
                error=str(e)
            )
            
            raise
```

### Q: å¦‚ä½•è®¾ç½®åˆé€‚çš„ç›‘æ§æŒ‡æ ‡ï¼Ÿ

A: éµå¾ªREDå’ŒUSEæ–¹æ³•è®ºï¼š

```python
# REDæŒ‡æ ‡ (Rate, Errors, Duration)
class REDMetrics:
    def __init__(self):
        self.request_rate = Counter('requests_per_second')
        self.error_rate = Counter('errors_per_second')
        self.request_duration = Histogram('request_duration_seconds')
    
    def record_request(self, duration: float, is_error: bool = False):
        self.request_rate.inc()
        self.request_duration.observe(duration)
        
        if is_error:
            self.error_rate.inc()

# USEæŒ‡æ ‡ (Utilization, Saturation, Errors)
class USEMetrics:
    def __init__(self):
        self.cpu_utilization = Gauge('cpu_utilization_percent')
        self.memory_utilization = Gauge('memory_utilization_percent')
        self.disk_utilization = Gauge('disk_utilization_percent')
        self.queue_saturation = Gauge('queue_length')
        self.system_errors = Counter('system_errors_total')
    
    def update_system_metrics(self):
        # æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
        self.cpu_utilization.set(psutil.cpu_percent())
        self.memory_utilization.set(psutil.virtual_memory().percent)
        self.disk_utilization.set(psutil.disk_usage('/').percent)
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [APIå¼€å‘æ¨¡å—ä½¿ç”¨æŒ‡å—](api.md)
- [æ•°æ®åº“æ¨¡å—ä½¿ç”¨æŒ‡å—](database.md)
- [ç¼“å­˜æ¶ˆæ¯æ¨¡å—ä½¿ç”¨æŒ‡å—](cache.md)
- [éƒ¨ç½²é…ç½®æ¨¡å—ä½¿ç”¨æŒ‡å—](deployment.md)
- [æ€§èƒ½ä¼˜åŒ–å»ºè®®](best-practices/performance.md)
- [å®‰å…¨å¼€å‘æŒ‡å—](best-practices/security.md)

---

å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ [GitHub Issues](https://github.com/your-username/python-backend-templates/issues) æˆ–æäº¤æ–°çš„é—®é¢˜ã€‚